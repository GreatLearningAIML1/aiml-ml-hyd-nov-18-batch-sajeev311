{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R7_InternalLab_Sajeev.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MyfMmMnPJjvn"
      },
      "source": [
        "## Train a simple convnet on the Fashion MNIST dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zjcGOJhcJjvp"
      },
      "source": [
        "In this, we will see how to deal with image data and train a convnet for image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jR0Pl2XjJjvq"
      },
      "source": [
        "### Load the  `fashion_mnist`  dataset\n",
        "\n",
        "** Use keras.datasets to load the dataset **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WoHKUIlxjedL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYx1ndvpjiCy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7b5b5e6f-c03d-41de-f87b-aaead6f5b0f2"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "import pickle\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import vis"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Qr75v_UYJjvs",
        "colab": {}
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxqYruaEjdeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hTI42-0qJjvw"
      },
      "source": [
        "### Find no.of samples are there in training and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g2sf67VoJjvx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6eebe064-f711-4516-e59b-6e59af24dc1b"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zewyDcBlJjv1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "79496f3e-2c29-4cb8-a5d2-f2750d0a99dd"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNPXpnnukw5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f88230f9-894d-4e35-de3f-9dc727a91597"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Eg0u2uUkw-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fe5e7dcb-701f-441e-bca6-223ebfcd3f10"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WytT2eRnJjv4"
      },
      "source": [
        "### Find dimensions of an image in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XycQGBSGJjv5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9526f1fd-ee74-4deb-8fd5-f7f8acf3b102"
      },
      "source": [
        "x_train[0].shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5jtdZ7RqJjv8"
      },
      "source": [
        "### Convert train and test labels to one hot vectors\n",
        "\n",
        "** check `keras.utils.to_categorical()` **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4zTj2YAmWq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_conv = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test_conv = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "y_train_class = keras.utils.to_categorical(y_train, 10)\n",
        "y_test_class = keras.utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sAD3q5I6Jjv9",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgHSCXy3JjwA",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xO5BRBzBJjwD"
      },
      "source": [
        "### Normalize both the train and test image data from 0-255 to 0-1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3fUQpMHxJjwE",
        "colab": {}
      },
      "source": [
        "x_train_conv =  x_train_conv.astype(\"float32\") / 255\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Okwo_SB5JjwI",
        "colab": {}
      },
      "source": [
        "x_test_conv = x_test_conv.astype(\"float32\") / 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "da5-DwgrJjwM"
      },
      "source": [
        "### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LPGVQ-JJJjwN",
        "colab": {}
      },
      "source": [
        "x_train_conv = x_train.reshape(x_train_conv.shape[0], 28, 28, 1)\n",
        "x_test_conv = x_test.reshape(x_test_conv.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OFRRTJq8JjwQ"
      },
      "source": [
        "### Import the necessary layers from keras to build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dWTZYnKSJjwR",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n",
        "from keras.layers import Convolution2D, MaxPooling2D,Conv2D\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "C18AoS7eJjwU"
      },
      "source": [
        "### Build a model \n",
        "\n",
        "** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DORCLgSwJjwV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "89900471-8ec5-42c3-dfed-43d696627dd8"
      },
      "source": [
        "model_simple_conv = Sequential()\n",
        "model_simple_conv.add(Conv2D(32, (3, 3), activation =\"relu\", input_shape=(28, 28, 1)))\n",
        "model_simple_conv.add(Conv2D(32, (3, 3), activation =\"relu\"))\n",
        "model_simple_conv.add(Flatten())\n",
        "model_simple_conv.add(Dense(128, activation='relu'))\n",
        "model_simple_conv.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_FJQR2ZpETq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "de446222-2d5a-444e-82c6-eefca5fc55c2"
      },
      "source": [
        "model_simple_conv.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 32)        9248      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 18432)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               2359424   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 2,370,282\n",
            "Trainable params: 2,370,282\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLSKsNdbpsIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_simple_conv.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA6fvtOLpsWw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "ca01015a-463b-4bba-e5d9-8a9f42430872"
      },
      "source": [
        "%%time \n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = [EarlyStopping(patience=5)]\n",
        "output_pooling_conv = model_simple_conv.fit(x_train_conv, y_train_class, batch_size=512, epochs=10, verbose=2, callbacks=early_stopping,\n",
        "                    validation_data=(x_test_conv, y_test_class))\n",
        "                              "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 6s - loss: 6.8339 - acc: 0.5081 - val_loss: 0.4776 - val_acc: 0.8276\n",
            "Epoch 2/10\n",
            " - 4s - loss: 0.3353 - acc: 0.8784 - val_loss: 0.3375 - val_acc: 0.8773\n",
            "Epoch 3/10\n",
            " - 4s - loss: 0.2161 - acc: 0.9201 - val_loss: 0.3411 - val_acc: 0.8842\n",
            "Epoch 4/10\n",
            " - 4s - loss: 0.1481 - acc: 0.9451 - val_loss: 0.3590 - val_acc: 0.8863\n",
            "Epoch 5/10\n",
            " - 4s - loss: 0.1054 - acc: 0.9618 - val_loss: 0.3730 - val_acc: 0.8908\n",
            "Epoch 6/10\n",
            " - 4s - loss: 0.0711 - acc: 0.9752 - val_loss: 0.4212 - val_acc: 0.8868\n",
            "Epoch 7/10\n",
            " - 4s - loss: 0.0475 - acc: 0.9841 - val_loss: 0.4348 - val_acc: 0.8865\n",
            "CPU times: user 17.8 s, sys: 6.86 s, total: 24.7 s\n",
            "Wall time: 29.5 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ju69vKdIJjwX"
      },
      "source": [
        "### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L2hAP94vJjwY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "474878ef-4b6a-4861-dead-58a39c52dfd5"
      },
      "source": [
        "model_pooling_conv = Sequential()\n",
        "model_pooling_conv.add(Conv2D(32, (3, 3), activation =\"relu\", input_shape=(28, 28, 1)))\n",
        "model_pooling_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_pooling_conv.add(Dropout(0.25))\n",
        "model_pooling_conv.add(Conv2D(32, (3, 3), activation =\"relu\"))\n",
        "model_pooling_conv.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model_pooling_conv.add(Dropout(0.25))\n",
        "model_pooling_conv.add(Flatten())\n",
        "model_pooling_conv.add(Dense(128, activation='relu'))\n",
        "model_pooling_conv.add(Dense(10, activation='softmax'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okumXKVZqVhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "d2a791bd-abc8-4d47-e715-a69030b065ca"
      },
      "source": [
        "model_pooling_conv.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 11, 11, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 800)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               102528    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 113,386\n",
            "Trainable params: 113,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzI20lqqqgZz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_pooling_conv.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99JbmMOFqgiZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 403
        },
        "outputId": "30cae437-156c-40aa-c25c-a9269d11a8f9"
      },
      "source": [
        "%%time \n",
        "from keras.callbacks import EarlyStopping\n",
        "output_pooling_conv = model_pooling_conv.fit(x_train_conv, y_train_class, batch_size=512, epochs=10, verbose=2, callbacks=early_stopping,\n",
        "                    validation_data=(x_test_conv, y_test_class))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            " - 3s - loss: 6.4410 - acc: 0.4591 - val_loss: 0.8659 - val_acc: 0.7031\n",
            "Epoch 2/10\n",
            " - 2s - loss: 0.7371 - acc: 0.7278 - val_loss: 0.5872 - val_acc: 0.7798\n",
            "Epoch 3/10\n",
            " - 2s - loss: 0.5783 - acc: 0.7834 - val_loss: 0.4898 - val_acc: 0.8180\n",
            "Epoch 4/10\n",
            " - 2s - loss: 0.5164 - acc: 0.8073 - val_loss: 0.4450 - val_acc: 0.8372\n",
            "Epoch 5/10\n",
            " - 2s - loss: 0.4758 - acc: 0.8237 - val_loss: 0.4165 - val_acc: 0.8472\n",
            "Epoch 6/10\n",
            " - 2s - loss: 0.4416 - acc: 0.8358 - val_loss: 0.4110 - val_acc: 0.8458\n",
            "Epoch 7/10\n",
            " - 2s - loss: 0.4172 - acc: 0.8477 - val_loss: 0.3783 - val_acc: 0.8637\n",
            "Epoch 8/10\n",
            " - 2s - loss: 0.3911 - acc: 0.8558 - val_loss: 0.3725 - val_acc: 0.8622\n",
            "Epoch 9/10\n",
            " - 2s - loss: 0.3709 - acc: 0.8613 - val_loss: 0.3479 - val_acc: 0.8707\n",
            "Epoch 10/10\n",
            " - 2s - loss: 0.3575 - acc: 0.8684 - val_loss: 0.3411 - val_acc: 0.8773\n",
            "CPU times: user 17.1 s, sys: 5.31 s, total: 22.4 s\n",
            "Wall time: 24.2 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lGTA3bfEJjwa"
      },
      "source": [
        "### Now, to the above model, lets add Data Augmentation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F6gX8n5SJjwb"
      },
      "source": [
        "### Import the ImageDataGenrator from keras and fit the training images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Cbz4uHBuJjwc",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kLU0k3Fqthu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "    samplewise_center=False,  # set each sample mean to 0\n",
        "    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "    samplewise_std_normalization=False,  # divide each input by its std\n",
        "    rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "    horizontal_flip=False,  # randomly flip images\n",
        "    vertical_flip=False)  # randomly flip images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NjcdV9qq-hb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen.fit(x_train_conv)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pl-8dOo7Jjwf"
      },
      "source": [
        "#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DpI1_McYJjwg",
        "outputId": "af67d1e0-96a7-4b91-bd2e-648e0034e05f",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "gen = datagen.flow(x_train_conv[0:1], batch_size=1)\n",
        "for i in range(1, 6):\n",
        "    plt.subplot(1,5,i)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(gen.next().squeeze(), cmap='gray')\n",
        "    plt.plot()\n",
        "plt.show()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dmPl5yE8Jjwm"
      },
      "source": [
        "### Run the above model using fit_generator()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44ZnDdJYJjwn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84068536-f8a2-46e7-86c1-86756602815f"
      },
      "source": [
        "x_train_conv[:1].shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0BuFjW3AsQkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen.fit(x_train_conv[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGc4Xo8NsTMz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = datagen.flow(x_train_conv[:1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBRP4PmasTSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#image = []\n",
        "#for i in range(3):\n",
        " #   img = samples.next()\n",
        " #   img = img.squeeze()\n",
        " #   image.append(vis.imshow(img))\n",
        "#image[0] | image[1] | image[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKVOeWBDsTWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MwQQW5iOJjwq"
      },
      "source": [
        "###  Report the final train and validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c1SrtBEPJjwq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "76f2a39a-518d-4bdc-e8d2-2ce35b11db74"
      },
      "source": [
        "model_pooling_conv.evaluate(x_test_conv,y_test_class)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 67us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3410657127141952, 0.8773]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBwVWNQC2qZD",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}